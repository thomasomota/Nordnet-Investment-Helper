{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from helpers import *\n",
    "from scraping import *\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "URL_FOND = \"https://www.nordnet.no/mux/web/fonder/sok.html?nn_kategori=&kategori=&forvaltare=&sokord=&sok=1&ppm=0&nobuy=&flik=&nm=&typ=1\"\n",
    "\n",
    "URL_DETAIL_PAGE_PREFIX = 'https://secust.msse.se/se/nordnetny/funds/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(URL_FOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_string = \"/mux/web/fonder/fondfakta.html?\"\n",
    "prepend_prefix = \"https://www.nordnet.no\"\n",
    "\n",
    "urls = get_urls(soup, regex=regex_string, prefix=prepend_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cost_dataframe(soup):\n",
    "    \n",
    "    table_soup = soup.find(\"h2\", text = \"Maks kjøpsavgift\").next_sibling.next_sibling\n",
    "\n",
    "    row_names = [\"Kjøp\", \"Selg\", \"Forvaltningsavgift\", \"Resultatbasert avgift(maks)\", \"Løpende kostnader\"]\n",
    "    td_tags = table_soup.findAll(\"td\", class_=None)\n",
    "\n",
    "    values = get_floats_as_strings_from_tags(td_tags)\n",
    "    columns = ['Kjøpsavgift', \"Salgsavgift\", \"Forvaltningsavgift\", \"Resultatbastert avgift(maks)\", \"Løpende kostnader\"]\n",
    "\n",
    "    return create_df(columns, values)\n",
    "\n",
    "def create_volatility_dataframe(soup):\n",
    "    \n",
    "    table_soup = soup.find(\"a\", text = \"Gj.snitt avkastning\").parent.parent.parent\n",
    "\n",
    "    row_names = [\"Gj.snitt avkastning\", \"Standardavvik\", \"Sharpe Ratio\"]\n",
    "    td_tags = table_soup.findAll(\"td\", class_=None)\n",
    "\n",
    "    values = get_floats_as_strings_from_tags(td_tags)\n",
    "    columns = [\"Gj.snitt avkastning\", \"Standardavvik\", \"Sharpe Ratio\"]\n",
    "\n",
    "    return create_df(columns, values)\n",
    "\n",
    "\n",
    "def get_overview_dataframe(overview_soup):\n",
    "    \n",
    "    dataFrame_costs = create_cost_dataframe(overview_soup)\n",
    "    dataFrame_volatility = create_volatility_dataframe(overview_soup)\n",
    "   \n",
    "    #CONCAT RESULTS\n",
    "    dataframe_fund = pd.concat([dataFrame_costs, dataFrame_volatility], axis=1)\n",
    "    fund_name = overview_soup.find(\"div\", id=\"container\").h1.text\n",
    "    dataframe_fund['Name'] = fund_name\n",
    "\n",
    "    return dataframe_fund\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 s ± 385 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_dataframe(urls):\n",
    "    main_dataframe = pd.DataFrame()\n",
    "    for url in urls:\n",
    "        detail_page_soup = get_soup(url)\n",
    "\n",
    "        url_postfix = detail_page_soup.find(lambda tag: tag.name == \"iframe\").get('src').split(\"overview\")[1]\n",
    "\n",
    "        url_portfolio_page = URL_DETAIL_PAGE_PREFIX + 'portfolio' + url_postfix\n",
    "        url_overview_page = URL_DETAIL_PAGE_PREFIX + 'overview' + url_postfix\n",
    "\n",
    "        portefolio_soup = get_soup(url_portfolio_page)    \n",
    "        overview_soup = get_soup(url_overview_page)\n",
    "\n",
    "        fund_dataframe = get_overview_dataframe(overview_soup)\n",
    "\n",
    "        main_dataframe = main_dataframe.append(fund_dataframe)\n",
    "\n",
    "%timeit -r3 create_dataframe(urls[:10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iiiish.. We have around 700 funds, and running through 7 of them takes us around 10 seconds. This gives us a rough estimate that it's going to take around 1000 seconds, or 16.6 minutes. Let's try to speed this up by using a lib called `SoupStrainer` that's going to help us parse less html, as i suspect it's this that takes the most effort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def get_soup_with_strainer(url, strainer):\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.text, 'lxml', parse_only=strainer) \n",
    "\n",
    "def get_overview_dataframe_with_soupstrainer(overview_soup):\n",
    "    \n",
    "    #COST TABLE\n",
    "    cost_table_soup = overview_soup.find(\"td\", text = \"Kjøp\").parent.parent\n",
    "    \n",
    "    cost_row_names = [\"Kjøp\", \"Selg\", \"Forvaltningsavgift\", \"Resultatbasert avgift(maks)\", \"Løpende kostnader\"]\n",
    "    cost_td_tags = cost_table_soup.findAll(\"td\", class_=None)\n",
    "\n",
    "    cost_values = get_floats_as_strings_from_tags(cost_td_tags)\n",
    "    cost_columns = ['Kjøpsavgift', \"Salgsavgift\", \"Forvaltningsavgift\", \"Resultatbastert avgift(maks)\", \"Løpende kostnader\"]\n",
    "\n",
    "    dataFrame_costs = create_df(cost_columns, cost_values)\n",
    "\n",
    "    #VOLATILITY TABLE\n",
    "    volatility_table_soup = overview_soup.find(\"td\", text = \"Gj.snitt avkastning\").parent.parent\n",
    "\n",
    "    vol_row_names = [\"Gj.snitt avkastning\", \"Standardavvik\", \"Sharpe Ratio\"]\n",
    "    vol_td_tags = volatility_table_soup.findAll(\"td\", class_=None)\n",
    "\n",
    "    vol_values = get_floats_as_strings_from_tags(vol_td_tags)\n",
    "    vol_columns = [\"Gj.snitt avkastning\", \"Standardavvik\", \"Sharpe Ratio\"]\n",
    "\n",
    "    dataFrame_volatility =  create_df(vol_columns, vol_values)\n",
    "        \n",
    "    #CONCAT RESULTS\n",
    "    return pd.concat([dataFrame_costs, dataFrame_volatility], axis=1)\n",
    "\n",
    "def get_fund_name_from_url(url):\n",
    "    \n",
    "    return url.split('nm=')[-1].replace(\"+\", \" \")\n",
    "\n",
    "def append_new_fund(main_dataframe, url):\n",
    "    only_iframe = SoupStrainer(\"iframe\")\n",
    "    detail_page_soup = get_soup_with_strainer(url, only_iframe)\n",
    "\n",
    "    url_postfix = detail_page_soup.find(lambda tag: tag.name == \"iframe\").get('src').split(\"overview\")[1]\n",
    "\n",
    "    url_portfolio_page = URL_DETAIL_PAGE_PREFIX + 'portfolio' + url_postfix\n",
    "    url_overview_page = URL_DETAIL_PAGE_PREFIX + 'overview' + url_postfix\n",
    "\n",
    "    only_tables = SoupStrainer(\"table\") \n",
    "\n",
    "    portefolio_soup = get_soup_with_strainer(url_portfolio_page, only_tables)    \n",
    "    overview_soup = get_soup_with_strainer(url_overview_page, only_tables)\n",
    "\n",
    "    fund_dataframe = get_overview_dataframe_with_soupstrainer(overview_soup)\n",
    "\n",
    "    fund_dataframe['Name'] = get_fund_name_from_url(url)\n",
    "\n",
    "    return main_dataframe.append(fund_dataframe)\n",
    "\n",
    "def create_dataframe_using_soupstrainer(urls):\n",
    "    main_dataframe = pd.DataFrame()\n",
    "    for url in urls:\n",
    "        \n",
    "        main_dataframe = append_new_fund(main_dataframe, url)\n",
    "        \n",
    "    return main_dataframe\n",
    "\n",
    "%timeit -r1 create_dataframe_using_soupstrainer(urls[:10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I shaved a whopping .0 seconds of the run with 10 funds, giving me a total profit of ish 35 seconds for the whole set! \n",
    "\n",
    "Not too impressive, but still we made an improvement, ish! \n",
    "\n",
    "Another thing we can try is to parallelize the task of getting the dataframe for each url ( corresponding to a fund ) \n",
    "\n",
    "Let's do that next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def create_fund_dataframe(url):\n",
    "    only_iframe = SoupStrainer(\"iframe\")\n",
    "    detail_page_soup = get_soup_with_strainer(url, only_iframe)\n",
    "\n",
    "    url_postfix = detail_page_soup.find(lambda tag: tag.name == \"iframe\").get('src').split(\"overview\")[1]\n",
    "\n",
    "    url_portfolio_page = URL_DETAIL_PAGE_PREFIX + 'portfolio' + url_postfix\n",
    "    url_overview_page = URL_DETAIL_PAGE_PREFIX + 'overview' + url_postfix\n",
    "\n",
    "    only_tables = SoupStrainer(\"table\") \n",
    "\n",
    "    portefolio_soup = get_soup_with_strainer(url_portfolio_page, only_tables)    \n",
    "    overview_soup = get_soup_with_strainer(url_overview_page, only_tables)\n",
    "\n",
    "    fund_dataframe = get_overview_dataframe_with_soupstrainer(overview_soup)\n",
    "    fund_dataframe['Name'] = get_fund_name_from_url(url)\n",
    "\n",
    "    return fund_dataframe\n",
    "\n",
    "def create_dataframe_in_parallel(urls, pool_size):\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    from multiprocessing import Pool\n",
    "    with Pool(pool_size) as p:\n",
    "        dataframes = p.map(create_fund_dataframe, urls)\n",
    "\n",
    "    for dataframe in dataframes:\n",
    "        result = result.append(dataframe)\n",
    "        \n",
    "    return result\n",
    "\n",
    "%timeit -r1 create_dataframe_in_parallel(urls, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A WHOPPING increase in efficiency! Look at that go! The whole set done in 42.1 seconds, that's acceptable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checkpoint Charlie </h2>\n",
    "\n",
    "We're now at a point where we have a dataframe with values we would like to show off to the world. But we should start thinking about how we'll serve this information to whoever wants it! \n",
    "\n",
    "The following notebook uses only the methods provided in `helpers.py` and `scraping.py` which is the methods found in this notebook to be the most efficient! \n",
    "\n",
    "See you in the next Notebook: `Analysing and enhancing our data` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
